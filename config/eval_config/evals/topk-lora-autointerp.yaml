name: topk-lora-autointerp
approach: decoder_similarity  # Options: decoder_similarity, hybrid_causal, both
layer_idx: 13
k: ${model.k}
r: ${model.r}

# Data collection settings
collect_activations: True
score_activations: True
seq_len: 2048
n_tokens: 10000000
batch_size: 5

# Approach-specific settings
decoder_similarity:
  enabled: True
  similarity_metric: cosine  # Options: cosine, dot, euclidean
  similarity_threshold: 0.5
  sample_every_n_tokens: 100
  cache_residuals: True
  cache_decoder_projections: True
  n_top_similar_positions: 100
  stream_during_collection: True
  prefer_streaming_results: True

hybrid_causal:
  enabled: True
  causal_effect_window: 10
  causal_trigger_window: 20
  compute_ablations: True
  ablation_method: zero  # Options: zero, mean, resample
  track_logit_changes: True
  measure_kl_divergence: True

# Delphi integration
delphi_integration:
  use_explainer: True
  use_scorer: True
  explainer_model: "Qwen/Qwen3-30B-A3B-Thinking-2507"
  scorer_types: [detection, recall]
  adapt_prompts_for_causal: True
  include_effect_examples: True
  max_examples_per_feature: 20

# Output settings
cache_root: "./cache/topk_lora_autointerp_streaming_similarity_correct_format"
save_frequency: 10000
generate_visualizations: True
export_format: both  # Options: json, pickle, both

build_eval_function:
  _target_: src.evals.topk_lora_auto_interp
