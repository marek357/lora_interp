size: "small"
lora:
  r: 8192
  bias: 'none'
  alpha: 16384
  # alpha: 256
  dropout: 0.0
  top_k_experiment: True
  # k: 128
  # k_final: 512
  k: 1024
  # k_final: 2
  k_final: 64
  # k_schedule: constant
  k_schedule: cubic
  temperature: 0.1
  temperature_final: 0.005
  temperature_schedule: linear
  layer: 18
  target_modules:
    - layers.18.self_attn.q_proj
    - layers.18.self_attn.k_proj
    - layers.18.self_attn.v_proj
    - layers.18.self_attn.o_proj
    - layers.18.mlp.gate_proj
    - layers.18.mlp.up_proj
    - layers.18.mlp.down_proj
